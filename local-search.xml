<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title></title>
    <link href="/2024/10/19/%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA/"/>
    <url>/2024/10/19/%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA/</url>
    
    <content type="html"><![CDATA[<h1 id="线性可分支持向量机与硬间隔最大化"><a href="#线性可分支持向量机与硬间隔最大化" class="headerlink" title="线性可分支持向量机与硬间隔最大化"></a>线性可分支持向量机与硬间隔最大化</h1><p>![[Pasted image 20241017112150.png]]<br>![[Pasted image 20241017112600.png]]</p><h1 id="线性支持向量机与软间隔最大化"><a href="#线性支持向量机与软间隔最大化" class="headerlink" title="线性支持向量机与软间隔最大化"></a>线性支持向量机与软间隔最大化</h1><h1 id="非线性支持向量机与核函数"><a href="#非线性支持向量机与核函数" class="headerlink" title="非线性支持向量机与核函数"></a>非线性支持向量机与核函数</h1><h1 id="序列最小最优化算法"><a href="#序列最小最优化算法" class="headerlink" title="序列最小最优化算法"></a>序列最小最优化算法</h1>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title></title>
    <link href="/2024/10/19/%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF%E6%B3%95/"/>
    <url>/2024/10/19/%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF%E6%B3%95/</url>
    
    <content type="html"><![CDATA[<p>#生成模型</p><blockquote><p>[!quote]<br>朴素贝叶斯(naive Bayes)法是基于<strong>贝叶斯定理</strong>与特征<strong>条件独立假设</strong>的分类方法。对于给定的训练数据集，首先基于特征条件独立假设学习输入输出的联合概率分布；然后基于此模型，对给定的输入$x$，利用贝叶斯定理求出后验概率最大的输出$y$。朴素贝叶斯法实现简单，学习与预测的效率都很高，是一种常用的方法。</p></blockquote><h1 id="朴素贝叶斯法的学习与分类"><a href="#朴素贝叶斯法的学习与分类" class="headerlink" title="朴素贝叶斯法的学习与分类"></a>朴素贝叶斯法的学习与分类</h1><h2 id="基本方法"><a href="#基本方法" class="headerlink" title="基本方法"></a>基本方法</h2><p>![[Pasted image 20240919101009.png]]<br>朴素贝叶斯法通过训练数据集学习联合概率分布$P(X,Y)$。具体地，学习以下先验概率分布及条件概率分布。<br><strong>先验概率分布</strong><br>$$P(Y&#x3D;c_k) ,k&#x3D;1,2,\cdots,K$$<br><strong>条件概率分布</strong><br>$$P(X&#x3D;x|Y&#x3D;c_{k})&#x3D;P(X^{(1)}&#x3D;x^{(1)},\cdots,X^{(n)}&#x3D;x^{(n)}|Y&#x3D;c_{k}),\quad k&#x3D;1,2,\cdots,K$$</p><p>![[Pasted image 20240919101624.png]]</p><p>朴素贝叶斯法对<strong>条件概率分布作了条件独立性的假设</strong>。由于这是一个较强的假设，朴素贝叶斯法也由此得名。具体地，条件独立性假设是<br>$$\begin{gathered}<br>P(X&#x3D;x|Y&#x3D;c_{k}) &#x3D;P(X^{(1)}&#x3D;x^{(1)},\cdots,X^{(n)}&#x3D;x^{(n)}|Y&#x3D;c_{k}) \<br>&#x3D;\prod_{j&#x3D;1}^nP(X^{(j)}&#x3D;x^{(j)}|Y&#x3D;c_k)<br>\end{gathered}$$<br>条件独立假设等于是说用于<strong>分类的特征在类确定的条件下都是条件独立的</strong>。这一假设使朴素贝叶斯法变得简单，但有时会牺牲一定的分类准确率。</p><p>朴素贝叶斯法分类时，对给定的输入$x$,通过学习到的模型计算后验概率分布$P(Y&#x3D;c_k|X&#x3D;x)$,将后验概率最大的类作为$x$的类输出。后验概率计算根据贝叶斯定理进行：<br>$$P(Y&#x3D;c_k|X&#x3D;x)&#x3D;\frac{P(X&#x3D;x|Y&#x3D;c_k)P(Y&#x3D;c_k)}{\sum_kP(X&#x3D;x|Y&#x3D;c_k)P(Y&#x3D;c_k)}$$<br>$$P(Y&#x3D;c_{k}|X&#x3D;x)&#x3D;\frac{P(Y&#x3D;c_{k})\prod_{j}P(X^{(j)}&#x3D;x^{(j)}|Y&#x3D;c_{k})}{\sum_{k}P(Y&#x3D;c_{k})\prod_{j}P(X^{(j)}&#x3D;x^{(j)}|Y&#x3D;c_{k})},\quad k&#x3D;1,2,\cdots,K$$<br>这是朴素贝叶斯法分类的基本公式。于是，朴素贝叶斯分类器可表示为<br>$$y&#x3D;f(x)&#x3D;\arg\max_{c_k}\frac{P(Y&#x3D;c_k)\prod_jP(X^{(j)}&#x3D;x^{(j)}|Y&#x3D;c_k)}{\sum_kP(Y&#x3D;c_k)\prod_jP(X^{(j)}&#x3D;x^{(j)}|Y&#x3D;c_k)}$$<br>分母是一定的，分类器可表示为：<br>$$y&#x3D;\arg\max_{c_k}P(Y&#x3D;c_k)\prod_jP(X^{(j)}&#x3D;x^{(j)}|Y&#x3D;c_k)$$</p><h2 id="后验概率最大化的含义"><a href="#后验概率最大化的含义" class="headerlink" title="后验概率最大化的含义"></a>后验概率最大化的含义</h2><blockquote><p>[!note]<br>朴素贝叶斯法将实例分到后验概率最大的类中。这等价于期望风险最小化。</p></blockquote><p><strong>期望风险函数：</strong><br>$$R_{\exp}(f)&#x3D;E[L(f(X),Y)]$$</p><p>假设损失函数为0-1损失函数：<br>$$\left.L(Y,f(X))&#x3D;\left{\begin{array}{cc}1,&amp;Y\neq f(X)\\0,&amp;Y&#x3D;f(X)\end{array}\right.\right.$$<br>$$R_{\exp}(f)&#x3D;E[L(f(X),Y)]&#x3D;P(Y\ne f(X))&#x3D;1-P(Y&#x3D;f(X))$$<br>$$P(Y&#x3D;f(X))$$</p><p>$$f(x)&#x3D;\mathrm{argmax}_{y\in\mathcal{Y}}P(Y&#x3D;f(X))$$<br>对于$X&#x3D;x$逐个极小化</p><p>$$f(x)&#x3D;\mathrm{argmax}_{y\in\mathcal{Y}}P(y&#x3D;c_k|X&#x3D;x)$$</p><p>这样一来，根据期望风险最小化准则就得到了后验概率最大化准则：</p><p>$$f(x)&#x3D;\arg\max_{c_k}P(c_k|X&#x3D;x)$$<br>![[Pasted image 20240919111344.png]]</p><h1 id="朴素贝叶斯法的参数估计"><a href="#朴素贝叶斯法的参数估计" class="headerlink" title="朴素贝叶斯法的参数估计"></a>朴素贝叶斯法的参数估计</h1><h2 id="极大似然估计"><a href="#极大似然估计" class="headerlink" title="极大似然估计"></a>极大似然估计</h2><p>在朴素贝叶斯法中，学习意味着估计$P(Y&#x3D;c_k)$和$P(X^{(j)}&#x3D;x^(j)|Y &#x3D;c_k)$。可应用极大似然估计法估计相应的概率。先验概率$P(Y&#x3D;c_k)$的极大似然估计是<br>![[Pasted image 20240920093632.png]]<br>![[Pasted image 20240920093753.png]]</p><h2 id="学习与分类算法"><a href="#学习与分类算法" class="headerlink" title="学习与分类算法"></a>学习与分类算法</h2><blockquote><p>[!note]  朴素贝叶斯算法(naive Bayes algorithm)<br>![[Pasted image 20240920093947.png]]<br>![[Pasted image 20240920094012.png]]<br>![[Pasted image 20240920094048.png]]</p></blockquote><blockquote><p>[!question]<br>![[Pasted image 20240920094318.png]]</p></blockquote><blockquote><p>[!done] 解：<br>$P(Y&#x3D;1)&#x3D;9&#x2F;15&#x3D;\frac{3}{5}$<br>$P(Y&#x3D;-1)&#x3D;\frac25$<br>$P(X^{(1)}&#x3D;1|Y&#x3D;1)&#x3D;2&#x2F;9$<br>$P(X^{(1)}&#x3D;2|Y&#x3D;1)&#x3D;1&#x2F;3$<br>$P(X^{(1)}&#x3D;3|Y&#x3D;1)&#x3D;4&#x2F;9$<br>$P(X^{(2)}&#x3D;S|Y&#x3D;1)&#x3D;1&#x2F;9$<br>$P(X^{(2)}&#x3D;M|Y&#x3D;1)&#x3D;4&#x2F;9$<br>$P(X^{(2)}&#x3D;L|Y&#x3D;1)&#x3D;4&#x2F;9$<br>$P(X^{(1)}&#x3D;2|Y&#x3D;1)&#x3D;1&#x2F;3$<br>$P(X^{(2)}&#x3D;S|Y&#x3D;1)&#x3D;1&#x2F;9$<br>$P(X^{(1)}&#x3D;2|Y&#x3D;-1)&#x3D;1&#x2F;3$<br>$P(X^{(2)}&#x3D;S|Y&#x3D;-1)&#x3D;1&#x2F;2$<br>$P(Y&#x3D;-1)P(X^{(1)}&#x3D;2|Y&#x3D;-1)P(X^{(2)}&#x3D;S|Y&#x3D;-1)&#x3D;2&#x2F;5 \times 1&#x2F;6 &#x3D;1&#x2F;15$<br>$P(Y&#x3D;1)P(X^{(1)}&#x3D;2|Y&#x3D;1)P(X^{(2)}&#x3D;S|Y&#x3D;1)&#x3D;3&#x2F;5 \times 1&#x2F;27 &#x3D;1&#x2F;45$<br>$y&#x3D;-1$</p></blockquote><h2 id="贝叶斯估计"><a href="#贝叶斯估计" class="headerlink" title="贝叶斯估计"></a>贝叶斯估计</h2><p>用极大似然估计可能会出现所要估计的概率值为0的情况。这时会影响到后验概率的计算结果，使分类产生偏差。解决这一问题的方法是采用贝叶斯估计。具体地，条件概率的贝叶斯估计是</p><p>$$P_{\lambda}(X^{(j)}&#x3D;a_{jl}|Y&#x3D;c_{k})&#x3D;\frac{\sum_{i&#x3D;1}^{N}I(x_{i}^{(j)}&#x3D;a_{jl},y_{i}&#x3D;c_{k})+\lambda}{\sum_{i&#x3D;1}^{N}I(y_{i}&#x3D;c_{k})+S_{j}\lambda}$$<br>![[Pasted image 20240920145228.png]]</p><blockquote><p>[!question]<br>![[Pasted image 20240920094318.png]]<br>按照拉普拉斯平滑估计概率</p></blockquote><blockquote><p>[!done]  解：<br>$\lambda &#x3D; 1$<br>先验概率：<br>$$P(Y&#x3D;1)&#x3D;\frac{9+1}{15+2}&#x3D;10&#x2F;17$$<br>$$P(Y&#x3D;-1)&#x3D;\frac{6+1}{15+2}&#x3D;7&#x2F;17$$<br>$$P(X^{(1)}&#x3D;2|Y&#x3D;1)&#x3D;\frac{3+1}{9+3}&#x3D;1&#x2F;3$$<br>$$P(X^{(1)}&#x3D;2|Y&#x3D;-1)&#x3D;\frac{2+1}{6+3}&#x3D;1&#x2F;3$$<br>$$P(X^{(2)}&#x3D;S|Y&#x3D;1)&#x3D;\frac{1+1}{9+3}&#x3D;1&#x2F;6$$<br>$$P(X^{(2)}&#x3D;S|Y&#x3D;-1)&#x3D;\frac{3+1}{6+3}&#x3D;4&#x2F;9$$<br>$P(Y&#x3D;-1)P(X^{(1)}&#x3D;2|Y&#x3D;-1)P(X^{(2)}&#x3D;S|Y&#x3D;-1)&#x3D;7&#x2F;17 \times 1&#x2F;3 \times 4&#x2F;9 &#x3D;0.0610$<br>$P(Y&#x3D;1)P(X^{(1)}&#x3D;2|Y&#x3D;1)P(X^{(2)}&#x3D;S|Y&#x3D;1)&#x3D;10&#x2F;17 \times 1&#x2F;3 \times 1&#x2F;6&#x3D; 0.0327$</p></blockquote>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title></title>
    <link href="/2024/10/19/%E9%80%BB%E8%BE%91%E6%96%AF%E8%B0%9B%E5%9B%9E%E5%BD%92%E4%B8%8E%E6%9C%80%E5%A4%A7%E7%86%B5%E6%A8%A1%E5%9E%8B/"/>
    <url>/2024/10/19/%E9%80%BB%E8%BE%91%E6%96%AF%E8%B0%9B%E5%9B%9E%E5%BD%92%E4%B8%8E%E6%9C%80%E5%A4%A7%E7%86%B5%E6%A8%A1%E5%9E%8B/</url>
    
    <content type="html"><![CDATA[<p>#分类 </p><blockquote><p>[!cite] <em>摘要</em><br><em>逻辑斯谛回归(logistic regression)是统计学习中的经典分类方法。最大熵是概率模型学习的一个准则，将其推广到分类问题得到最大熵模型(maximum entropy model)。逻辑斯谛回归模型与最大熵模型都属于对数线性模型。</em></p></blockquote><h1 id="逻辑斯谛回归模型"><a href="#逻辑斯谛回归模型" class="headerlink" title="逻辑斯谛回归模型"></a>逻辑斯谛回归模型</h1><h2 id="logistic分布"><a href="#logistic分布" class="headerlink" title="logistic分布"></a>logistic分布</h2><blockquote><p>[!note]  logistic分布<br>设$X$是连续随机变量，$X$服从逻辑斯谛分布是指$X$<br>具有下列分布函数和密度函数：<br>$$F(x)&#x3D;P(X\leqslant x)&#x3D;\frac{1}{1+\mathrm{e}^{-(x-\mu)&#x2F;\gamma}}$$<br>$$f(x)&#x3D;F’(x)&#x3D;\frac{\mathrm{e}^{-(x-\mu)&#x2F;\gamma}}{\gamma(1+\mathrm{e}^{-(x-\mu)&#x2F;\gamma})^2}$$式中，$\mu$为位置参数，$\gamma&gt;0$为形状参数。</p></blockquote><h2 id="二项逻辑斯谛回归模型"><a href="#二项逻辑斯谛回归模型" class="headerlink" title="二项逻辑斯谛回归模型"></a>二项逻辑斯谛回归模型</h2><p>二项逻辑斯谛回归模型(binomial logistic regression model)是一种分类模型，由条件概率分布$P(Y|X)$表示，形式为参数化的逻辑斯谛分布。这里，随机变量$X$取值为实数，随机变量$Y$取值为$1$或$0$。我们通过监督学习的方法来估计模型参数。</p><blockquote><p>[!note]<br>二项逻辑斯谛回归模型是如下的条件概率分布：$$P(Y&#x3D;1|x)&#x3D;\frac{\exp(w\cdot x+b)}{1+\exp(w\cdot x+b)}$$<br>$$P(Y&#x3D;0|x)&#x3D;1-P(Y&#x3D;1|x)&#x3D;\frac{1}{1+\exp(w\cdot x+b)}$$<br>这里，$x\in\mathbf{R}^n$是输入，$Y\in{0,1}$是输出，$w\in\mathbf{R}^n$和$b\in\mathbf{R}$是参数，$w$称为权值向<br>量，$b$称为偏置，$w\cdot x$为$w$和$x$的内积。</p></blockquote><p>一个事件的<strong>几率(odds)</strong> 是指该事件发生的概率与该事件不发生的概率的比值。如果事件发生的概率是$p$,那么该事件的几率是$\frac{p}{1-p}$ ,该事件的<strong>对数几率(log odds)</strong> 或logit函数是：<br>$$logit(p)&#x3D;\log\frac p{1-p}$$<br>对于logistic回归，对数几率为<br>$$\log\frac{P(Y&#x3D;1|x)}{1-P(Y&#x3D;1|x)}&#x3D;w\cdot x+b$$<br>在逻辑斯谛回归模型中，输出$Y&#x3D;1$的对数几率是输入$x$的线性函数。或者说，输出$Y&#x3D;1$的对数几率是由输入$x$的线性函数表示的模型，即逻辑斯谛回归模型。</p><h2 id="模型参数估计"><a href="#模型参数估计" class="headerlink" title="模型参数估计"></a>模型参数估计</h2><p>对于给定的训练数据集$T&#x3D;{(x_1,y_1),(x_2,y_2),\cdots$, $(x_N,y_N)}$,其中，$x_i\in\mathbf{R}^n,y_i\in{0,1}$，可以应用极大似然估计法估计模型参数，从而得到逻辑斯谛回归模型。</p><p>设：<br>$$P(Y&#x3D;1|x)&#x3D;\pi(x),\quad P(Y&#x3D;0|x)&#x3D;1-\pi(x)$$<br>似然函数为：<br>$$\prod_{i&#x3D;1}^N[\pi(x_i)]^{y_i}[1-\pi(x_i)]^{1-y_i}$$<br>对数似然函数为<br>$$\begin{aligned}L(w)&amp;&#x3D;\sum_{i&#x3D;1}^N\left[y_i\log\pi(x_i)+(1-y_i)\log(1-\pi(x_i))\right]\&amp;&#x3D;\sum_{i&#x3D;1}^N\left[y_i\log\frac{\pi(x_i)}{1-\pi(x_i)}+\log(1-\pi(x_i))\right]\&amp;&#x3D;\sum_{i&#x3D;1}^N\left[y_i(w\cdot x_i)-\log(1+\exp(w\cdot x_i)\right]\end{aligned}$$对$L(w)$求极大值，得到$w$的估计值。</p><h2 id="多项logistic回归"><a href="#多项logistic回归" class="headerlink" title="多项logistic回归"></a>多项logistic回归</h2><p>推广为多项逻辑斯谛回归模型(multi-nominal logistic regression model)，用于多类分类。假设离散型随机变量$Y$ 的取值集合是 ${1,2,\cdots,K}$,那么多项逻辑斯谛回归模型是<br>$$P(Y&#x3D;k|x)&#x3D;\frac{\exp(w_{k}\cdot x)}{1+\sum_{k&#x3D;1}^{K-1}\exp(w_{k}\cdot x)},\quad k&#x3D;1,2,\cdots,K-1$$<br>$$P(Y&#x3D;K|x)&#x3D;\frac{1}{1+\sum_{k&#x3D;1}^{K-1}\exp(w_{k}\cdot x)}$$</p><p>这里，$x\in\mathbf{R}^n+1,w_k\in\mathbf{R}^{n+1}$。</p><p>二项逻辑斯谛回归的参数估计法也可以推广到多项逻辑斯谛回归。</p><h1 id="最大熵模型"><a href="#最大熵模型" class="headerlink" title="最大熵模型"></a>最大熵模型</h1><h2 id="最大熵原理"><a href="#最大熵原理" class="headerlink" title="最大熵原理"></a>最大熵原理</h2><blockquote><p>[!cite] <em>最大熵原理是概率模型学习的一个准则。最大熵原理认为，学习概率模型时，在所有可能的概率模型(分布)中，熵最大的模型是最好的模型。通常用约束条件来确定概率模型的集合，所以，最大熵原理也可以表述为在满足约束条件的模型集合中选取熵最大的模型。</em></p></blockquote><p><strong>香农信息内容：</strong> The Shannon information content of an outcome $x$ is defined to be<br>$$h(x)&#x3D;\log_2\frac{1}{p(x)}$$<br>measured in <strong>bits</strong> </p><p><strong>一个$X$全体的熵：</strong>  离散随机变量$X$</p><p>$$H(X)&#x3D;\sum_{x\in \mathcal{A}_X} P(x)\log\frac{1}{P(x)}$$</p><ul><li>$H(X)\ge0$</li><li>当概率向量$\mathbf{p}$是均匀分布时，$H(X)$取最大，最大值为$\log(|\mathcal{A}_X|)$<br>  证明：根据Jensen’s不等式</li></ul><p><strong>相对熵&#x2F;KL散度：</strong><br>between two probability distributions $P(x)$ and $Q(x)$ that are defined over the same alphabet $\mathcal{A}<em>X$<br>$$D</em>{KL}(P|Q)&#x3D;\sum_x P(x)\log\frac{Q(x)}{P(x)}$$<br>The relative entropy satisfies Gibbs’ inequality：<br>$$D_{KL}(P|Q)\ge0$$<br><strong>最大熵原理：</strong><br>最大熵原理认为要选择的概率模型首先必须满足已有的事实，即约束条件。在没有更多信息的情况下，那些不确定的部分都是”等可能的”。最大熵原理通过熵的最大化来表示等可能性。</p><h2 id="最大熵模型的定义"><a href="#最大熵模型的定义" class="headerlink" title="最大熵模型的定义"></a>最大熵模型的定义</h2><p>考虑到前述皆为离散，应用到分类模型中。<br>假设分类模型为条件概率分布·<br>![[Pasted image 20241013174235.png]]</p><blockquote><p>[!note] 条件熵<br>设有随机变量$(X,Y)$,其联合概率分布为$$P(X&#x3D;x_i,Y&#x3D;y_j)&#x3D;p_{ij},\quad i&#x3D;1,2,\cdots,n;\quad j&#x3D;1,2,\cdots,m$$条件熵$H(Y|X)$表示在已知随机变量$X$的条件下随机变量$Y$的不确定性。随机变量$X$给定的条件下随机变量$Y$的条件熵(conditional entropy)$H(Y|X)$,定义为$X$给定条件下$Y$的条件概率分布的熵对$X$的数学期望$$H(Y|X)&#x3D;\sum_{i&#x3D;1}^np_iH(Y|X&#x3D;x_i)$$</p></blockquote>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title></title>
    <link href="/2024/10/19/k%E8%BF%91%E9%82%BB%E6%B3%95/"/>
    <url>/2024/10/19/k%E8%BF%91%E9%82%BB%E6%B3%95/</url>
    
    <content type="html"><![CDATA[<p>#分类 #回归 </p><h1 id="k近邻算法"><a href="#k近邻算法" class="headerlink" title="k近邻算法"></a>k近邻算法</h1><pre><code class="hljs">给定一个训练数据集，对新的输入实例，在训练数据集中找到与该实例最邻近的k个实例，这k个实例的多数属于某个类，就把该输入实例分为这个类。</code></pre><blockquote><p>[!note] k近邻法<br>![[Pasted image 20240918152714.png]]<br>![[Pasted image 20240918152747.png]]</p></blockquote><pre><code class="hljs">k近邻法的特殊情况是k=1的情形，称为最近邻算法。对于输入的实例点(特征向量)x,最近邻法将训练数据集中与x最邻近点的类作为x的类。</code></pre><h1 id="k近邻模型"><a href="#k近邻模型" class="headerlink" title="k近邻模型"></a>k近邻模型</h1><p>k近邻法使用的模型实际上对应于<strong>对特征空间的划分</strong>。模型由三个基本要素——<strong>距离度量</strong>、<strong>k值的选择</strong>和<strong>分类决策规则</strong>决定。</p><h2 id="模型"><a href="#模型" class="headerlink" title="模型"></a>模型</h2><p>k近邻法中，当训练集、距离度量(如欧氏距离)、k值及分类决策规则(如多数表决)确定后，对于任何一个新的输入实例，它所属的类唯一地确定。这相当于根据上述要素将特征空间划分为一些子空间，确定子空间里的每个点所属的类。<br>特征空间中，对每个训练实例点$x_j$，距离该点比其他点更近的所有点组成一个区域，叫作<strong>单元(cell)</strong> 每个训练实例点拥有一个单元，所有训练实例点的单元构成对特征空间的一个划分。最近邻法将实例$x_i$的类$y_i$作为其单元中所有点的类标记(class label)。这样，每个单元的实例点的类别是确定的。</p><h2 id="距离度量"><a href="#距离度量" class="headerlink" title="距离度量"></a>距离度量</h2><p>![[Pasted image 20240918154333.png]]<br>![[Pasted image 20240918154348.png]]<br>![[Pasted image 20240918154402.png]]</p><h2 id="k值的选择"><a href="#k值的选择" class="headerlink" title="k值的选择"></a>k值的选择</h2><p>![[Pasted image 20240918155959.png]]<br>    理解为模型误差与方差的关系。</p><ul><li>近似误差随着假设空间增大而减小，估计误差随着假设空间增大而增大；</li><li>估计误差不仅与假设空间有关，还与训练集的大小有关，训练集越大则找到的经验风险最小化函数性能越好；</li><li>近似误差则单纯与假设空间有关。</li></ul><blockquote><p>[!cite]<br>近似误差是指你模型的Bias，比如你把明明是非线性的关系用线性回归模型去近似，就存在近似误差。它是一定有偏的，这个Bias的Scale也许会远远小于你需要的预测精度。<br>当模型没有近似误差或偏差时，估计误差就是指模型的Variance。这个Variance，会随着样本量的提升而减小到零。这样的估计也称作相合估计（统计上）或一致估计（经济上）。最近，学界更关注于模型的Regularization，也就是正则化。研究人员发现对高维数据，如果致力于模型的Unbiasedness，那么其Variance就会很大；降低模型的Variance，则不得不舍弃模型的Unbiasedness，引入一定的偏差。这便是Tradeoff of Bias and Variance。常用的正则化方法是L1和L2正则化。</p></blockquote><h2 id="分类决策规则"><a href="#分类决策规则" class="headerlink" title="分类决策规则"></a>分类决策规则</h2><p>k近邻法中的分类决策规则往往是多数表决，即由输入实例的k个邻近的训练实例中的多数类决定输入实例的类。</p><p><strong>多数表决规则(majority voting rule)</strong> 有如下解释：如果分类的损失函数为0-1损失函数，分类函数为<br>$$f:\mathbf{R}^n\to{c_1,c_2,\cdots,c_K}$$<br>误分类的概率为：<br>$$P(Y\neq f(X))&#x3D;1-P(Y&#x3D;f(X))$$<br>对于实例$x\in \mathcal{X}$，最近邻的k个训练实例点构成的集合为$N_k(x)$ ，若涵盖此点集的类别为$c_j$ ，误分类率为$$\frac{1}{k}\sum_{x_i\in N_k(x)}I(y_i\neq c_j)&#x3D;1-\frac{1}{k}\sum_{x_i\in N_k(x)}I(y_i&#x3D;c_j)$$<br>要<strong>使误分类率最小即经验风险最小</strong>，就要使$\sum_{x_i\in N_k(x)}I(y_i&#x3D;c_j)$最大，所以多数表决规则等价于经验风险最小化。</p><h1 id="k近邻法的实现：kd树"><a href="#k近邻法的实现：kd树" class="headerlink" title="k近邻法的实现：kd树"></a>k近邻法的实现：kd树</h1><h2 id="构造kd树"><a href="#构造kd树" class="headerlink" title="构造kd树"></a>构造kd树</h2><p>![[Pasted image 20240918164856.png]]</p><h2 id="搜索kd树"><a href="#搜索kd树" class="headerlink" title="搜索kd树"></a>搜索kd树</h2><p>![[Pasted image 20240919084808.png]]</p><p>求$x&#x3D;(3,4.5)^T$ 的最近邻点</p><p>![[Pasted image 20240919092819.png]]<br>![[Pasted image 20240919093203.png]]<br>![[Pasted image 20240919093507.png]]</p>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title></title>
    <link href="/2024/10/19/08-%E6%8F%90%E5%8D%87%E6%96%B9%E6%B3%95/"/>
    <url>/2024/10/19/08-%E6%8F%90%E5%8D%87%E6%96%B9%E6%B3%95/</url>
    
    <content type="html"><![CDATA[<h1 id="AdaBoost"><a href="#AdaBoost" class="headerlink" title="AdaBoost"></a>AdaBoost</h1><h1 id="提升树"><a href="#提升树" class="headerlink" title="提升树"></a>提升树</h1><p>提升树是以分类树或回归树为基本分类器的提升方法。提升树被认为是统计学习中性能最好的方法之一。</p><p>![[Pasted image 20241015211734.png]]</p>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>Hello World</title>
    <link href="/2024/10/17/hello-world/"/>
    <url>/2024/10/17/hello-world/</url>
    
    <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo new <span class="string">&quot;My New Post&quot;</span></span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/writing.html">Writing</a></p><h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/server.html">Server</a></p><h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/generating.html">Generating</a></p><h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p>]]></content>
    
    
    
  </entry>
  
  
  
  
</search>
